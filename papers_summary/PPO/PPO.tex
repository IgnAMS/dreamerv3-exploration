\documentclass[11pt,a4paper]{article}

% --------------------
% Paquetes básicos
% --------------------
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage[english]{babel}

\usepackage{amsmath, amssymb}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{cite}


% Code
\usepackage{caption}
\usepackage{xcolor}
\usepackage{listings}
\renewcommand{\lstlistingname}{Algorithm}
\definecolor{codebg}{rgb}{0.95,0.95,0.95}
\definecolor{keyword}{rgb}{0.13,0.29,0.53}
\definecolor{comment}{rgb}{0,0.5,0}
\definecolor{string}{rgb}{0.58,0,0.82}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{codebg},
    basicstyle=\ttfamily\footnotesize,
    keywordstyle=\color{keyword}\bfseries,
    commentstyle=\color{comment}\itshape,
    stringstyle=\color{string},
    numbers=left,
    numberstyle=\tiny,
    stepnumber=1,
    frame=single,
    breaklines=true,
    tabsize=2,
    showstringspaces=false
}

\lstset{style=mystyle}

% Márgenes
\usepackage{geometry}


\renewcommand{\thesubsection}{\arabic{subsection}}
\setcounter{secnumdepth}{1} % Comentar para que aparezcan los números en los subsections 

\geometry{margin=2.5cm}

% Espaciado
\usepackage{setspace}
\onehalfspacing

\title{\textbf{Summary of the paper:}\\
\large Proximal Policy Optimization Algorithms}

\author{Ignacio Monardes}

\date{\today}

\begin{document}

\maketitle

\section*{Referencia}
\noindent
\textbf{Authors:} John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov \\
\textbf{Title:} \textit{Proximal Policy Optimization Algorithms} \\
\textbf{Conference/Journal:}  \\
\textbf{Link:} \url{https://arxiv.org/pdf/1707.06347}

\section{Motivation}
\subsection{Current Problem} 

TRPO is very complex to implement in many scenarios and bad with noisy environments. 
Deep Q-Learning is difficult is not very good on many simple and continuous tasks.

There is room for a robust, scalable and efficient model.

\subsection{Relevance of the problem}

An scalable algorithm for many environments and easy to implement.

\subsection{Hole in the literature / Previous limitations}


\section{Main Idea}
\subsection{Reinforce Gradient}

First of all, the expected value to maximize is:
\[
J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} [ R(\tau) ]
= \sum_{\tau} p_\theta(\tau) R(\tau)
\]

We can notice that:
\[
\nabla_\theta p(\tau) = p(\tau) \nabla_\theta \log p(\tau) 
\]

Demonstration:
\[
\nabla_\theta \log p(\tau) = \frac{\nabla_\theta p(\tau)}{p(\tau)}
\Rightarrow
\nabla_\theta p(\tau) = p(\tau)\nabla_\theta \log p(\tau)
\]

The probability of a trajectory $\tau$ is defined by:
\begin{align*}
    p_\theta(\tau) &= p(s_0) \prod_{t} \pi_\theta(a_t \mid s_t) p(s_{t+1} \mid s_t, a_t)   \\
    \log p_\theta(\tau) &= \log p(s_0) + \sum_{t} \log \pi_\theta(a_t \mid s_t) + \sum_t \log p(s_{t+1} \mid s_t, a_t)   \\
    \nabla_\theta \log p_\theta(\tau) &= \sum_{t} \nabla_\theta \log \pi_\theta(a_t \mid s_t) 
\end{align*}

Then:
\[
\nabla_\theta J(\theta) = \mathbb{E}\left[
    \sum_t \nabla_\theta \log \pi_\theta(a_t \mid s_t) G_t
\right]
\]

We can make rest a bias and define the advantage $A(s_t, a_t) := Q(s_t, a_t) - V(s_t)$, resulting in:
\[
\hat{g} = \hat{\mathbb{E}}_t\left[ 
    \nabla_\theta \log \pi_\theta(a_t \mid s_t) \hat{A}_t
\right]
\]

\subsection{TRPO}
TRPO maximizes this problem:
\begin{align*}
    \max_\theta &\quad  \hat{\mathbb{E}}_t \left[ \frac{ \pi_\theta(a_t \mid s_t)}{ \pi_{\theta_\text{old}}(a_t \mid s_t) } \hat{A}_t \right] \\
    \text{subject to} & \quad \hat{\mathbb{E}}_t[KL[\pi_{\theta_\text{old}} (\cdot \mid s_t), \pi_\theta(\cdot \mid s_t)]] \leq \delta
\end{align*}
with a hard constraint over the restriction.

Theorically we could use the restriction on the function with a coefficient $\beta$:
\begin{align*}
    \max_\theta &\quad  \hat{\mathbb{E}}_t \left[ \frac{ \pi_\theta(a_t \mid s_t)}{ \pi_{\theta_\text{old}}(a_t \mid s_t) } \hat{A}_t - \beta KL[\pi_{\theta_\text{old}} (\cdot \mid s_t), \pi_\theta(\cdot \mid s_t)] \right]
\end{align*}
this would make a lower bound of the value.

The problem here is that $\beta$ shouldn't be fixed and with additional modfications to Stochastic Gradient Descent.


\subsection{Innovation: CLIP, Adaptative KL}
We define the ratio $r_t(\theta) := \frac{\pi_\theta(a_t \mid s_t)}{ \pi_{\theta_\text{old}}(a_t \mid s_t)}$.

TRPO maximizes a surrogate objective:
\[
L^{CPI}(\theta) = \hat{\mathbb{E}}_t\left[ 
    \frac{\pi_\theta(a_t \mid s_t)}{ \pi_{\theta_\text{old}}(a_t \mid s_t)} \hat{A}_t 
 \right] = \hat{\mathbb{E}}_t[r_t(\theta) \hat{A}_t]
\]
where CPI refers to \textit{Conservative Policy Iteration}. But the gradient could be very large. So we clipped it.
\[
L^{CLIP}(\theta) = \hat{\mathbb{E}}\left[ \min\left(
    r_t(\theta)\hat{A}_t, clip(r_t(\theta), 1-\varepsilon, 1+\varepsilon) \hat{A}_t
\right) \right]
\]
This allows to move the gradient in no more than the range $[1 - \varepsilon, 1 + \varepsilon]$, and as we take the minnimum
we create a lower bound.
\begin{center}
    \includegraphics[width=\linewidth]{pasted-images/PPO-16-28-55.png}
\end{center}

\subsection{KL Penalty coeff}
Another option to the clipped objective is to use a penalty on KL divergence. 
This performed worse than the clipped version. 

\section{Methodology}
The methodology is just to adapt the Loss function to the desired loss with clipped or other thing and use stochastci gradient ascent.
We could use a learned state value $V(s)$ or the finite-horizon estiamters. If our neural network shares parameters between the policy and 
value function, we must use a loss function that uses both of them. We can add exploration by using entropy bonus. 

We finally have this loss function:
\[
L_t^{CLIP+VF+S}(\theta) := \hat{\mathbb{E}}_t\left[
    L_t^{CLIP}(\theta) + c_1 L_t^{VF}(\theta) + c_2 S[\phi_\theta](s_t)
\right]
\]
where $S$ is an entropy bonus and $L_t^{VF}$ is a squared error loss.

It is very used an estimator of the advantage as:
\begin{align*}
    &\hat{A}_t := \delta_t + (\gamma \lambda) \delta_{t+1} + \dots + \dots + (\gamma \lambda)^{T-t+1} \delta_{T-1} \\
    &\text{where } \quad \delta_t = r_t + \gamma V(s_{t+1}) - V(s_t)
\end{align*}
with $T << \text{Length of the episode}$.



\subsection{Modeling} 
\begin{lstlisting}[language=Python, caption={Pseudo code PPO}]
for i in range(n_iters):
    for j in range(n_envs):
        buffer, next_reward_AC = run_policy(steps=T)
        advantages = compute_advantages(buffer, next_reward_AC)
    # M <= NT
    L = loss(epochs=K, batch_size=M)
    weights_old = weights
\end{lstlisting}

\subsection{Assumptions}

\subsection{Architecture}
The policy is made up of a fully connected MLP with two hidden layers of 64 units, and tanh nonlinearities, outputting the mean of a Gaussian distribution.
The parameters of the policy and the value function were not shared. There is no entropy bonus. 


\section{Experiments}

\subsection{Baselines}
The baselines are:
\begin{itemize}
    \item TRPO.
    \item Cross-entropy method (CEM).
    \item Vanilla policy gradient with adaptative stepsize.
    \item A2C (Advantage Actor Critic, which is synchronous A3C).
\end{itemize}

\subsection{Environments or datasets}
They tested the problem with 7 simulated robotics tasks in MuJoCo physics engine. For each task they do 1M timesteps of training. 
The tasks were runned with 4 different seeds. The score is the average reward of the last 100 episodes. The scores are normalized to the scale [0, 1] and averaged over 21 runs. 

\begin{center}
    \includegraphics[width=\linewidth]{pasted-images/PPO-11-33-30.png}
\end{center}
PPO outperforms almos all the continuos control environments compared to the others algorithms.

PPO works well on humanoid robotics tasks:
\begin{center}
    \includegraphics[width=\linewidth]{pasted-images/PPO-11-39-04.png}
\end{center}




\subsection{Settings}


\subsection{New Scenarios}


\section{Results}


\subsection{¿En qué escenarios funciona mejor / peor?}

\section{Discusión Crítica}
\subsection{Fortalezas}

\subsection{Debilidades} 

\subsection{Supuestos cuestionables} 

\subsection{Qué no queda claro} 

\section{Conclusiones}

\section*{Comentario Personal}
\subsection{¿Es una buena contribución?}

\subsection{¿Lo usaría en mi investigación?}


\bibliographystyle{plain}
\bibliography{references}

\end{document}
